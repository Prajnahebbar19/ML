# -*- coding: utf-8 -*-
"""1BM22CS121__Lab-2(A)-DataProcessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-aDAKZ_SS8K1PYJE1bLQqdytX0RFrrOT
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
import numpy as np
import seaborn as sns

# Load the dataset into a pandas DataFrame
df = pd.read_csv('housing.csv')

# Display descriptive statistics
df.describe()

# Display information about the DataFrame
df.info()

# Plot the histogram of each feature(Indicate what does histogram
# indicate on median_income and house_median_age)


# Plot histograms for each numerical feature
df.hist(bins=50, figsize=(20, 15))
plt.show()

# Interpretation of median_income and house_median_age histograms
print("Interpretation of Histograms:")
print("Median Income:")
print("The histogram of median income shows the distribution of income levels in the dataset.")
print("The median of the distribution can be approximated by observing the peak or the mode of the histogram.")
print("The histogram reveals the skewness and kurtosis, providing information on the overall income distribution's shape.")
print()
print("House Median Age:")
print("The histogram of house median age shows the distribution of ages of houses in the dataset.")
print("The median of the distribution can be approximated by observing the peak or the mode of the histogram.")
print("The histogram reveals the skewness and kurtosis, providing insights into whether the houses are mostly new or old, and the spread of house ages in the region.")

import matplotlib.pyplot as plt
df.hist(column="median_income", bins=30, figsize=(8,6))
plt.show()

import matplotlib.pyplot as plt
df.hist(column="median_house_value", bins=30, figsize=(8,6))
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit

# Load the dataset
housing = pd.read_csv('housing.csv')

# For this demonstration, consider only 'median_income' and 'median_house_value'
housing_selected = housing[['median_income', 'median_house_value']].copy()

# Random split: This splits the data randomly without preserving any specific distribution.
train_set_random, test_set_random = train_test_split(housing_selected, test_size=0.2, random_state=42)

# Explanation:
# - The stratified split creates an 'income_cat' column that bins median_income into 5 categories.
# - StratifiedShuffleSplit then splits the data such that the proportion of each income category
#   is maintained in both the train and test sets.
# - This method ensures that key distribution characteristics (like income) are preserved,
#   reducing the risk of sampling bias compared to a random split.

print("Random Split:")
print("Random Train Set (first 5 rows):")
train_set_random.head()

print("Random Test Set (first 5 rows):")
test_set_random.head()

# Explanation:
# In a random split, the test set is created by randomly selecting 20% of the data.
# While simple, this method may lead to unbalanced distributions of key attributes (e.g., median_income).

# For stratified sampling, first create an income category.
housing_selected['income_cat'] = pd.cut(housing_selected['median_income'],
                                        bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                                        labels=[1, 2, 3, 4, 5])

# Use StratifiedShuffleSplit to ensure the income distribution is preserved in both sets.
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing_selected, housing_selected['income_cat']):
    strat_train_set = housing_selected.loc[train_index]
    strat_test_set = housing_selected.loc[test_index]

# Remove the temporary income category attribute.
for dataset in (strat_train_set, strat_test_set):
    dataset.drop("income_cat", axis=1, inplace=True)

print("Stratified Split:")
print("Stratified Train Set (first 5 rows):")
strat_train_set.head()

print("Stratified Test Set (first 5 rows):")
strat_test_set.head()

import matplotlib.pyplot as plt
housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4,
             s=housing["population"]/100, label="population", figsize=(7,5),
             c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,)
plt.legend()

# Drop the non-numeric column for correlation analysis
housing_numeric = housing.drop("ocean_proximity", axis=1)

def find_correlation(housing_numeric):
    # Compute the correlation matrix
    corr_matrix = housing_numeric.corr()
    # Return the correlations with median_house_value, sorted descending
    return corr_matrix["median_house_value"].sort_values(ascending=False)

# Compute correlation coefficients
cor_coef = find_correlation(housing_numeric)
print("Correlation Coefficient:\n", cor_coef)

# ----------------------------
# Plot 1: Bar Graph of Correlations
# ----------------------------
plt.figure(figsize=(10,6))
sns.barplot(x=cor_coef.index, y=cor_coef.values, palette='viridis')
plt.xticks(rotation=45)
plt.xlabel("Features")
plt.ylabel("Correlation Coefficient")
plt.title("Correlation of Features with Median House Value")
plt.tight_layout()
plt.show()

# ----------------------------
# Identify the Most Correlated Feature (besides median_house_value itself)
# ----------------------------
# The first entry is median_house_value itself (correlation = 1), so take the second item.
max_feature = cor_coef.index[1]
print("\nFeature with maximum correlation (besides median_house_value itself):", max_feature)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8,6))
# Differentiate by using 'housing_median_age' for the color
scatter = plt.scatter(housing_numeric[max_feature],
                      housing_numeric["median_house_value"],
                      alpha=0.5,
                      c=housing_numeric["housing_median_age"],
                      cmap='viridis',
                      edgecolor='k')
plt.xlabel(max_feature)
plt.ylabel("Median House Value")
plt.title(f"{max_feature} vs. Median House Value\n(Color indicates housing median age)")
# Add a colorbar to explain the color mapping
cbar = plt.colorbar(scatter)
cbar.set_label("Housing Median Age")
plt.tight_layout()
plt.show()

# Load the dataset
housing = pd.read_csv('housing.csv')

# Create new combined features
housing["rooms_per_household"] = housing["total_rooms"] / housing["households"]
housing["bedrooms_per_room"] = housing["total_bedrooms"] / housing["total_rooms"]
housing["population_per_household"] = housing["population"] / housing["households"]

# Compute new correlation coefficients for these engineered features
corr_matrix = housing.drop("ocean_proximity", axis=1).corr()
print("Correlation with median_house_value:\n", corr_matrix["median_house_value"].sort_values(ascending=False))

# Plot a heatmap to see the updated correlations
plt.figure(figsize=(12,8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix with Engineered Features")
plt.show()

from sklearn.impute import SimpleImputer

# Select numeric features for cleaning
housing_numeric = housing.drop("ocean_proximity", axis=1)

# Check for missing values
print("Missing values before cleaning:\n", housing_numeric.isnull().sum())

# Impute missing values using the median
imputer = SimpleImputer(strategy="median")
housing_numeric_imputed = pd.DataFrame(imputer.fit_transform(housing_numeric),
                                       columns=housing_numeric.columns)

# Verify that missing values have been handled
print("\nMissing values after cleaning:\n", housing_numeric_imputed.isnull().sum())

from sklearn.preprocessing import OneHotEncoder

# Extract the categorical attribute
housing_cat = housing[["ocean_proximity"]]

# Perform one-hot encoding
encoder = OneHotEncoder()
housing_cat_1hot = encoder.fit_transform(housing_cat).toarray()

# Create a DataFrame for the encoded features
housing_cat_1hot_df = pd.DataFrame(housing_cat_1hot,
                                   columns=encoder.get_feature_names_out(["ocean_proximity"]))
housing_cat_1hot_df.head()

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler

# Custom transformer to add engineered attributes
class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room=True):
        self.add_bedrooms_per_room = add_bedrooms_per_room
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        # Assumes X is a NumPy array with the following columns:
        # total_rooms (index 3), total_bedrooms (index 2), population (index 4), households (index 5)
        rooms_per_household = X[:, 3] / X[:, 5]
        population_per_household = X[:, 4] / X[:, 5]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, 2] / X[:, 3]
            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]

# Identify numerical and categorical columns
num_attribs = housing.drop("ocean_proximity", axis=1).columns  # All numeric columns
cat_attribs = ["ocean_proximity"]

# Build numerical pipeline: impute missing values, add new attributes, then scale
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")),
    ('attribs_adder', CombinedAttributesAdder()),
    ('std_scaler', StandardScaler()),
])

# Build the full pipeline combining numerical and categorical processing
full_pipeline = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", OneHotEncoder(), cat_attribs),
])

# Process the dataset using the pipeline
housing_prepared = full_pipeline.fit_transform(housing)
print("Shape of processed data:", housing_prepared.shape)

