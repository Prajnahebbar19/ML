# -*- coding: utf-8 -*-
"""1BM22CS121__Lab-1-ImportExport.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uowTHtQPp9zkEYu4bmwkVvuE5lMqY3Da
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder

# Define a function to cap outliers using the IQR method
def cap_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    return df

"""# **DIABETES DATASET**"""

from google.colab import drive
drive.mount('/content/drive')

# 1. Load the Diabetes dataset
diabetes_df = pd.read_csv("diabetes.csv")
print("=== Diabetes Dataset: Original Data ===")
diabetes_df.head()

# 2. Data Cleaning
# 2.a Handling Missing Values
# Check missing values (assumed as NaN)
print("\nMissing values in Diabetes dataset:")
print(diabetes_df.isnull().sum())

# For numerical columns: fill missing values with mean
num_cols = diabetes_df.select_dtypes(include=[np.number]).columns.tolist()
for col in num_cols:
    diabetes_df[col].fillna(diabetes_df[col].mean(), inplace=True)

# For categorical columns: fill missing values with mode
cat_cols = diabetes_df.select_dtypes(include=['object']).columns.tolist()
for col in cat_cols:
    diabetes_df[col].fillna(diabetes_df[col].mode()[0], inplace=True)

# 2.c Handling Outliers for numerical columns
for col in num_cols:
    diabetes_df = cap_outliers(diabetes_df, col)

# 3. Data Transformations

# Create copies for each scaling technique
diabetes_minmax = diabetes_df.copy()
diabetes_standard = diabetes_df.copy()

# Apply Min-Max Scaling
minmax_scaler = MinMaxScaler()
diabetes_minmax[num_cols] = minmax_scaler.fit_transform(diabetes_minmax[num_cols])

# 1. Load the Adult Income dataset
adult_income_df = pd.read_csv("adult.csv")
print("\n=== Adult Income Dataset: Original Data ===")
print(adult_income_df.head())

# Apply Standard Scaling
standard_scaler = StandardScaler()
diabetes_standard[num_cols] = standard_scaler.fit_transform(diabetes_standard[num_cols])

print("\n=== Diabetes Dataset after Preprocessing (Min-Max Scaled) ===")
print(diabetes_minmax.head())

print("\n=== Diabetes Dataset after Preprocessing (Standard Scaled) ===")
print(diabetes_standard.head())

# 2. Data Cleaning
# 2.a Handling Missing Values
# In this dataset, missing values might be represented by "?".
adult_income_df.replace("?", np.nan, inplace=True)
print("\nMissing values after replacing '?' with NaN:")
print(adult_income_df.isnull().sum())

# For numerical columns: fill missing values with mean
num_cols_adult = adult_income_df.select_dtypes(include=[np.number]).columns.tolist()
for col in num_cols_adult:
    adult_income_df[col].fillna(adult_income_df[col].mean(), inplace=True)

# For categorical columns: fill missing values with mode
cat_cols_adult = adult_income_df.select_dtypes(include=['object']).columns.tolist()
for col in cat_cols_adult:
    adult_income_df[col].fillna(adult_income_df[col].mode()[0], inplace=True)

# 2.b Handling Categorical Data
# Encode categorical variables using One-Hot Encoding
adult_income_encoded = pd.get_dummies(adult_income_df, columns=cat_cols_adult)
print("\n=== Adult Income Dataset after One-Hot Encoding ===")
print(adult_income_encoded.head())

# 2.c Handling Outliers for numerical columns in the encoded dataset
for col in num_cols_adult:
    adult_income_encoded = cap_outliers(adult_income_encoded, col)

# 3. Data Transformations

# Create copies for each scaling technique
adult_income_minmax = adult_income_encoded.copy()
adult_income_standard = adult_income_encoded.copy()

# Apply Min-Max Scaling
adult_income_minmax[num_cols_adult] = minmax_scaler.fit_transform(adult_income_minmax[num_cols_adult])

# Apply Standard Scaling
adult_income_standard[num_cols_adult] = standard_scaler.fit_transform(adult_income_standard[num_cols_adult])

print("\n=== Adult Income Dataset after Preprocessing (Min-Max Scaled) ===")
adult_income_minmax.head()

print("\n=== Adult Income Dataset after Preprocessing (Standard Scaled) ===")
adult_income_standard.head()



# prompt: For both the datasets Diabetes and Adult income
# 1. Which columns in the dataset had missing values? How did you
# handle them ?
# 2. Which categorical columns did you identify in the dataset? How did
# you encode them ?
# 3. What is the difference between Min-Max Scaling and
# Standardization? When would you use one over the other?

# 1. Missing Values
# The code already handles missing values.
# For the Diabetes dataset, it fills numerical missing values with the mean
# and categorical missing values with the mode.
# For the Adult Income dataset, it first replaces "?" with NaN, then fills
# numerical missing values with the mean and categorical with the mode.

# 2. Categorical Columns and Encoding
# Diabetes Dataset:
# The 'Outcome' column is identified as categorical and encoded using Label Encoding.

# Adult Income Dataset:
# All identified categorical columns are encoded using One-Hot Encoding.  The code automatically detects and handles the categorical columns.

# 3. Min-Max Scaling vs. Standardization

# Difference:
# Min-Max Scaling scales the data to a specific range (usually 0 to 1).  It's sensitive to outliers.
# Standardization (Z-score normalization) transforms data to have a mean of 0 and a standard deviation of 1. It's less sensitive to outliers.

# When to use which:
# Min-Max Scaling is preferred when the algorithm is sensitive to the magnitude of the features (e.g., KNN, K-Means).  Use it when you know the data's distribution doesn't have significant outliers.
# Standardization is generally preferred if the data has a Gaussian-like distribution or when the algorithm is not sensitive to the feature scales (e.g., linear regression, logistic regression). Use it when you're concerned about outliers influencing the scaling.  Standardization is often more robust.

# prompt: For both the datasets Diabetes and Adult income
# 1. Which columns in the dataset had missing values? How did you
# handle them ?
# 2. Which categorical columns did you identify in the dataset? How did
# you encode them ?
# 3. What is the difference between Min-Max Scaling and
# Standardization? When would you use one over the other?
# also with code for each questions 1, 2, 3, not the entire

# 1. Missing Values (Diabetes Dataset)
print("\nMissing values in Diabetes dataset:")
print(diabetes_df.isnull().sum())
# Explanation: The code checks for missing values (NaN) in each column of the diabetes dataset and prints the count of missing values for each column.  Numerical columns are filled with the mean of the column, and categorical columns with the mode.

# 2. Categorical Columns and Encoding (Adult Income Dataset)
# Identify categorical columns
cat_cols_adult = adult_income_df.select_dtypes(include=['object']).columns.tolist()
print("\nCategorical columns in Adult Income dataset:", cat_cols_adult)
# Explanation:  This part identifies categorical columns in the adult income dataset.
# One-Hot encoding is then applied to these columns using pd.get_dummies.


# 3. Min-Max Scaling vs. Standardization
# (Explanation provided in the original code comments)

# Example usage of MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
# Assuming 'numerical_features' is a list of your numerical columns
numerical_features = ['age', 'bmi']  # Example, replace with actual column names
minmax_scaler = MinMaxScaler()
scaled_data = minmax_scaler.fit_transform(diabetes_df[numerical_features])
print(scaled_data)


# Example usage of StandardScaler
from sklearn.preprocessing import StandardScaler
standard_scaler = StandardScaler()
scaled_data_standard = standard_scaler.fit_transform(diabetes_df[numerical_features])
scaled_data_standard

